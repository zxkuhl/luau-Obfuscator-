<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Local AI in Browser</title>
  <script src="https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@latest"></script>
</head>
<body>
  <h1>Local AI Chat (runs on your CPU/GPU)</h1>
  <textarea id="input" rows="3" placeholder="Ask something..."></textarea><br>
  <button onclick="generate()">Send</button>
  <pre id="output"></pre>

  <script>
    const engine = new webllm.MLCEngine();

    async function initialize() {
      const reply = await engine.chat.completions.create({
        messages: [{ role: "system", content: "You are a helpful AI." }],
        model: "Llama-3.2-3B-Instruct-q4f16_1-MLC",  // or "Phi-3.5-mini-instruct-q4f16_1-MLC", etc.
        stream: false  // change to true for streaming
      });
      console.log("Model ready");
    }

    async function generate() {
      const prompt = document.getElementById("input").value;
      document.getElementById("output").textContent = "Thinking...\n";

      const reply = await engine.chat.completions.create({
        messages: [{ role: "user", content: prompt }],
        model: "Llama-3.2-3B-Instruct-q4f16_1-MLC",
        stream: true,
        temperature: 0.7
      });

      let text = "";
      for await (const chunk of reply) {
        const delta = chunk.choices[0]?.delta?.content || "";
        text += delta;
        document.getElementById("output").textContent = text;
      }
    }

    initialize();  // loads model on page load (several GB first time!)
  </script>
</body>
</html>
